<html><head><link href="/simplecss/styles.css" rel="stylesheet"/></head></html><!DOCTYPE html>






DataStage OSH Script - Data Warehousing Data Warehousing



















































<a class="skip-link screen-reader-text" href="index.html#content">Skip to content</a>







Reading Time:  3 minutes
<p>The IBM InfoSphere DataStage and QualityStage Designer client creates IBM InfoSphere DataStage jobs that are compiled into parallel job flows, and reusable components that execute on the parallel Information Server engine. It allows you to use familiar graphical point-and-click techniques to develop job flows for extracting, cleansing, transforming, integrating, and loading data into target files, target systems, or packaged applications.</p>
<p>The Designer generates all the code. It generates the OSH (Orchestrate SHell Script) and C++ code for any Transformer stages used.Briefly, the Designer performs the following tasks:* Validates link requirements, mandatory stage options, transformer logic, etc.* Generates OSH representation of data flows and stages (representations offramework ‚Äúoperators‚Äù).* Generates transform code for each Transformer stage which is then compiledinto C++ and then to corresponding native operators.* Reusable BuildOp stages can be compiled using the Designer GUI or fromthe command line.</p>

<p>The osh command is the main program of the InfoSphere Parallel Engine. This command is used by DataStage to perform several different tasks including parallel job execution and dataset management. Normally, there is no need to run this command directly but sometimes it is useful to use it for troubleshooting purposes.</p>
<p>To run this command there are 3 environment variables that must be set. These are:</p>
<ol type="1"><li>APT_ORCHHOME should point to Parallel Engine location</li><li>APT_CONFIG_FILE should point to a configuration file</li><li>LD_LIBRARY_PATH should include the path to the parallel engine libraries.</li></ol>
<p>Please note that the name of this environment variable may take a different name (such as LIBPATH in AIX or SLIB_PATH in HP-UX) depending on your Operating System. Note: This variable does not need to be set in Windows environments.</p>
<p>A short introduction to OSH:</p>
<ul><li>OSH is a powerful PX-Engine Orchestrate Shell which can be used to perform various operations including debugging DataStage jobs to see what is happening without using clients.</li><li>OSH uses the familiar syntax of the UNIX shell. such as Operator name,schema, operator options (‚Äú-name value‚Äù format), input (indicated by n&lt; where n is the input#), and output (indicated by the n&gt; where n is the output #).</li><li>Comment blocks introduce each operator, the order of which is determined bythe order stages were added to the canvas.</li><li>Virtual data sets (in memory native representation of data links) aregenerated to connect operators.</li><li>For every operator, input and/or output data sets are numbered sequentiallystarting from zero.</li></ul>
<p>Framework (Information Server Engine) terms and DataStage terms have equivalency. The GUI frequently uses terms from both paradigms. Runtime messages use framework terminology because the framework engine is where execution occurs. The following list shows the equivalency between framework and DataStage terms:* Schema corresponds to table definition* Property corresponds to format* Type corresponds to SQL type and length* Virtual data set corresponds to link* Record/field corresponds to row/column* Operator corresponds to stage</p>
<p>Note: The actual execution order of operators is dictated by input/output designators, and not by their placement on the diagram. The data sets connect the OSH operators. These are ‚Äúvirtual data sets‚Äù, that is, in memory data flows. Link names are used in data set names ‚Äî it is therefore good practice to give the links meaningful names.</p>

<h3>Meet Ananth Tirumanur. Hi there üëã</h3>

    <h4>I work on projects in data science, big data, data engineering, data modeling, software engineering, and system design.</h4>

    <ul>
        <li>üë®‚Äçüíª All of my projects are available at <a href="https://github.com/akrish1982">https://github.com/akrish1982</a></li>
        <li>üí¨ Ask me about <strong>Data engineering, SQL, Databases, Data pipelines, Data infrastructure</strong></li>
        <li>üìÑ My work history: <a href="https://www.linkedin.com/in/ananthtirumanur/">https://www.linkedin.com/in/ananthtirumanur/</a></li>
        <li>‚ö° Fun fact: Marathoner & Casual Birding enthusiast</li>
    </ul>

    <h3>Connect with me:</h3>
    <ul>
        <li>Twitter: <a href="https://twitter.com/akrish82">@akrish82</a></li>
        <li>LinkedIn: <a href="https://linkedin.com/in/ananthtirumanur/">https://linkedin.com/in/ananthtirumanur/</a></li>
    </ul>

<h3>My Resources:</h3>
    <ul>
        <li>LinkedIn Newsletter: <a href="https://www.linkedin.com/newsletters/data-engineering-with-aws-7096111313352880128/">Data Engineering with AWS</a></li>
        <li>Udemy Course: <a href="https://www.udemy.com/course/aws-certified-data-engineer-associate-practice-test/learn/quiz/6218524#overview">AWS Certified Data Engineer Associate Practice Test</a></li>
        <li>Python Crash Course: <a href="https://akrish82.gumroad.com/l/python-crash-course">Python Crash Course on Gumroad</a></li>
    </ul>

    <h3>Languages and Tools:</h3>
    <p>AWS, Bash, Cassandra, Django, Docker, Elasticsearch, Flask, Git, Go, Grafana, Hadoop, Hive, Hugo, Kafka, Kubernetes, Linux, MariaDB, MySQL, Pandas, PostgreSQL, Python, Redis, Scala, Scikit-learn, SQLite</p>





 





 
 







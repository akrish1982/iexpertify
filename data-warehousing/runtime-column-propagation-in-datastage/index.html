<html><head><link href="../../simplecss/styles.css" rel="stylesheet"/>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3040480045347797"
     crossorigin="anonymous"></script>
</head></html><!DOCTYPE html>






Runtime Column Propagation in DataStage - Data Warehousing Data Warehousing


<p>WHAT IS RCP IN DATASTAGE?</p>
<p>InfoSphere DataStage is also flexible about meta data. It can handle the situation in case meta data is not fully defined.</p>
<p>When we send the data from source to the target, sometimes we need to send only required columns. You can define part of your schema and specify that, if your job encounters additional columns that are not defined in the meta data when it actually runs, it will adopt these extra columns and propagate them through the rest of the job.  Which is called as Runtime Column Propagation (RCP).</p>
<p>RCP can be enabled for a project via the Administrator client, and set for individual links via the Output Page Columns tab for most stages, or in the Output page General tab for Transformer stages.</p>
<p>RCP Enable/Disable done at:</p>
<p>Project level:  in Administrator project properties </p>
<p>Job level: Job properties General tab</p>
<p>Stage/s:  Link Output Column tab</p>

<p>If run time column propagation is enabled in the DataStage Administrator, you can select the Run time column propagation to specify that columns encountered by a stage in a parallel job can be used even if they are not explicitly defined in the meta data. You should always ensure that run time column propagation is turned on if you want to use schema files to define column meta data.</p>
<p>Run time column propagation is used in case of partial schema usage. When we only know about the columns to be processed and we want all other columns to be propagated to target as they are.</p>
<p>USING RCP WITH SEQUENTIAL STAGES</p>
<p>Runtime column propagation (RCP) allows DataStage to be flexible about the columns you define in a job.</p>
<p>If RCP is enabled for a project, you can just define the columns you are interested in using in a job, but ask DataStage to propagate the other columns through the various stages.</p>
<p>So such columns can be extracted from the data source and end up on your data target without explicitly being operated on in between.</p>
<p>Sequential files, unlike most other data sources, do not have inherent column definitions, and so DataStage cannot always tell where there are extra columns that need propagating.</p>
<p>You can only use RCP on sequential files if you have used the Schema File property to specify a schema which describes all the columns in the sequential file.</p>
<p>You need to specify the same schema file for any similar stages in the job where you want to propagate columns. Stages that will require a schema file are:</p>
<p>Sequential File</p>
<p>File Set</p>
<p>External Source</p>
<p>External Target</p>
<p>Column Import</p>
<p>Column Export</p>

<br><br>
<h3>Meet Ananth Tirumanur. Hi there 👋</h3>

    <h4>I work on projects in data science, big data, data engineering, data modeling, software engineering, and system design.</h4>

    <ul>
        <li>👨‍💻 All of my projects are available at <a href="https://github.com/akrish1982">https://github.com/akrish1982</a></li>
        <li>💬 Ask me about <strong>Data engineering, SQL, Databases, Data pipelines, Data infrastructure</strong></li>
        <li>📄 My work history: <a href="https://www.linkedin.com/in/ananthtirumanur/">https://www.linkedin.com/in/ananthtirumanur/</a></li>
        <li>⚡ Fun fact: Marathoner & Casual Birding enthusiast</li>
    </ul>

    <h3>Connect with me:</h3>
    <ul>
        <li>Twitter: <a href="https://twitter.com/akrish82">@akrish82</a></li>
        <li>LinkedIn: <a href="https://linkedin.com/in/ananthtirumanur/">https://linkedin.com/in/ananthtirumanur/</a></li>
    </ul>

<h3>My Resources:</h3>
    <ul>
        <li>LinkedIn Newsletter: <a href="https://www.linkedin.com/newsletters/data-engineering-with-aws-7096111313352880128/">Data Engineering with AWS</a></li>
        <li>Udemy Course: <a href="https://www.udemy.com/course/aws-certified-data-engineer-associate-practice-test/learn/quiz/6218524#overview">AWS Certified Data Engineer Associate Practice Test</a></li>
        <li>Python Crash Course: <a href="https://akrish82.gumroad.com/l/python-crash-course">Python Crash Course on Gumroad</a></li>
    </ul>

    <h3>Languages and Tools:</h3>
    <p>AWS, Bash, Cassandra, Django, Docker, Elasticsearch, Flask, Git, Go, Grafana, Hadoop, Hive, Hugo, Kafka, Kubernetes, Linux, MariaDB, MySQL, Pandas, PostgreSQL, Python, Redis, Scala, Scikit-learn, SQLite</p>





 





 
 






